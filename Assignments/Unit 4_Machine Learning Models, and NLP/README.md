<h1>Tokenization summary</h1>
  
On our first day, we learned about word embeddings and tokenizing text with spacy so that we could use them in a natural language processing model. The first thing we did was download a pre-trained natural language model from spacy called en_core_web_md which is a downloadable version of the pre-trained model. We then use spacy.load() to load the model as a variable NLP. This is the medium size model but there are large and small versions as well. Once we download the spacy tool kit in collab we need to restart the runtime otherwise the notebook won't be able to recognize it's downloaded.

We now need to download the data and decide which column we want to use for training our model. The example data set that we’re using is one consisting of amazon reviews where the column we want is the string of the review and the column we want to predict is the number of stars the reviewer gave the item. However to be able to predict the stars our model needs to be able to understand what our text is saying. We can do this in several ways, one of which we have already briefly gone over, and is called text vectorization. This is where we use a text vectorization algorithm to convert the words or sentence into its own set of values that the model can be trained on. We could train a classification model on these values so that it might be able to predict who wrote the review (for example).  

To be able to vectorize each word correctly we need to ensure that our text is as ‘clean’ as possible. To make this happen we need to limit the number of unhelpful words in our text. This includes eliminating punctuation, stop words, and words that have little meaning or influence within our text. Stop words, such as ‘and, before, it, too, was, like, and that,’ need to be removed as they will skew our results since they are so common. Thankfully we have a module from spacy that will help us remove common stop words that it has already compiled. If we would like, we can join that provided list with a list of our own to include specific words from our text. An example of this might include the name of the main character from a book since we already know that character's name and it doesn't really add any value to our text. In total, the changes we need to make in our text include removing stop words, and punctuation as well as making sure that the case of each word is lowercase. This is because our model will interpret words that have a different case as two different words.

This process is called Tokenizing and can be done with the spacy module ‘tokenizer’ or it can be done manually with python and regex. For our spacy tokenizer, It still needs to be fed the pre-trained model so that it knows what to look for in your text so we will allow it to look at the en_core_web_lg model. Once the tokenizer has read the data from the model we can then pass in new data, such as our reviews, and it will process the data to the best of its ability. 

When processing text our model will not be able to distinguish two words that mean the same thing but where one is plural and one is singular. This is where lemmatization comes in. The process of lemmatization is where we use the spacy model to turn plural words into singular words so that it may better understand the text. This is fairly easy to do as once the model is imported and read, we can say ‘for every word (token) in this text, return the lemma of that word’ and our model will know what we mean. Another version of this that is less methodical is a process called stemming which does basically the same thing as lemmatization but only converts the plural words into singular words where lemmatization changes the whole word into the past tense version of the word. For example, lemmatization would turn ‘I was reading’ into ‘I, be, read’ and stemming would turn ‘sticks, ponies, bounced’ into ‘stick, poni, bounce’
  
 <h1> Text Representation </h1> 
 
 Now that we have the process for turning raw text into its most basic and significant form we need a way to represent our text in a way that the model can use for calculations and associations. This is where text vectorization comes in. We have briefly touched on this before but the basis of this process is to turn the unique information available in a sentence or word into a list of float values that the model can interpret. The model has been trained on a significant enough amount of data that given most English word vectors it can tell you exactly what word it was originally before it was vectorized. To take this to the next level of usefulness we will be converting a document (a large amount of text) into a vector that represents the document and also a set of vectors that represents each word within the text. 

The possible ways we can do this range from very simple to very complicated and the quality of the results of these processes will be directly related to the complexity of our model. An example of a very simple process of vectorization is called a Binary Bit vector. This is where two sentences that may or may not contain the same words are compared. In the resulting Array, the rows represent each sentence and the columns represent every word from each sentence, and the value at their connection in the array is a 1 if that sentence contains the word and 0 if it does not. This is called word vectorization using One hot encoding.

One issue with the way that this data is represented is that it has a large amount of Dimensionality. There would be a column for every single unique word in the document. To reduce this, we could instead count the number of instances of unique words per document. This representation is called Bag Of Words. By representing the words in a document in this way we can more accurately determine if two documents are similar by measuring the word frequency and word similarity within each document.

One of the more important processes for determining the significance of words within a document is called TF-IDF which stands for Term Frequency - Inverse Document Frequency. This process is important because it better represents the significance of a word compared to just a Term Frequency model which only represents how often a word appears. The way it does this is by multiplying each term frequency against the inverse of the total document frequency. Essentially what this does is adds a significance score to each word in each document where if a word is common across multiple documents but uncommon in a single document it will have a better representation of how important that word is. Terms that are more frequent within a document and across many documents will be penalized and have lower significance scores whereas words that are uncommon within documents and across all documents will be represented as having higher significance within their own document. 

Understanding why the format in which we represent the words/tokens within a series of documents is important for training more complex models. Up to this point, we have been trying to represent each specific word in numerical format but when we only focus on representing each word by itself, we lose any information about the context for the word. Think of sorting every word used in a book alphabetically and then training the model on that set of words. It would be just as accurate as predicting similarity to another book if it wasn't sorted. Each instance of the word is solely represented by itself. But what if we could change that? Instead, we will focus on representing the context of each word as a vector. We will represent this as a vector where each row is the words of the document and the columns are the derived contexts within the document. Each word would have a score of how related we think the word is to the context. This allows us to reduce dimensionality while retaining the maximum amount of information. This is the basis of word embeddings. Essentially by representing how much each word in the document is related to the contexts of the document we can represent semantic similarity as a vector.

There are 2 common models that can do this type of semantic similarity vectorization, the first is Word2Vec, which learns contexts from the text by using a ‘Continuous Bag of Words model, and the second SkipGram, which does the opposite and infers context from each word. These models still need to be told how many dimensions they should look for in each document and can be done by tuning the specific hyperparameter. However, when completed, it will be difficult to interpret the contexts in the model by looking at the specific words in the context. Though because of its highly specific representation of contexts within a document, this model is excellent for accurately comparing the similarity between two large documents.

<h1>Neural Network Basics Summary</h1>

In this Unit we began to explore Neural networks and the types of problems they can be used to solve. Simple machine learning algorithms work best on problems of classification which makes them useful for identification of complex patterns such as voice recognition, image recognition, text prediction and many others. We started by exploring natural language processing so that we could further understand how these algorithms work on a basic level before exploring more complicated topics.

The most recognizable Natural language processing is the autocorrect or text prediction that keeps us from making spelling errors while texting or writing. It is fairly simple to imagine how a model is trained to recognize spelling mistakes. If we think of a word as a pattern of letters, take for example the word 'fish', the word itself is made of 4 unique letters in a unique order. As humans we immediately know what this word says and means because we have associated the word with the sound each letter makes and then with the animal. This concept is essentially what we need to teach our model to do. To do this we first we have to teach the model what the correct spelling (or pattern) of every word so that it can recognize when it doesn't know a pattern. Once it sees that we have made a word it doesn't recognize, the next step is compare the unknown word to its dataset and find the closest matching word. Maybe you only missed 1 letter, 'Fush' is not an English word but we can see how it easily could be recognized as a misspelling of 'Fish'.

In most applications machine learning is just pattern matching and classification. The 5 major components of a Neural network are Neurons, Weight and Bias parameters, an activation function, a loss function and input/output layers. To further our understanding we explored the MNIST hand written digit dataset. This dataset consists of many 28x28 pixel images of a hand written digits between 0 - 9. To begin to match the patterns in this dataset we need to understand how the first layer of a neural network functions. This layer is called the "input layer" and is a collection of "Neurons" which is a single number usually between -1 and 1. Each Neuron is responsible for reacting to a single piece of data. This means that for our 28x28 pixel images our first layer needs to have 784 neurons, one for each pixel or piece of data. Each following layer is called a Hidden Layer. The last layer in the network is called the Output layer and needs to have as many neurons as we have classification options for our images. In this case we have images with numbers ranging from 0-9 so we have 10 neurons in our output layer.

How does the network know what each image represents? good question, in the beginning it doesn't. This is where we train the network to recognize patterns. In classification problems also called supervised learning, we have already classified some of the images in our dataset. When we feed the network an image it accepts the data into its input layer, then, surprisingly, creates a bunch of random noise. This random noise results in an incoherent output which is where weights and activation functions come in. To teach the network which number each image is supposed represent, the network takes each random output and adjusts the weights for each neuron in the hidden layers till the output matches our classified image. The network does this for every single image in the dataset and creates a extremely complex pattern that can recognize new images it has never seen before with a relatively decent accuracy.

Now lets talk about activation functions and how it effects the neurons in the network. An activation function is a function that is used to decide at what point each neuron "activates" and passes along data. For example if we had a neuron whose value was 0.49 and our activation function was set up so that any number below .50 was 'Off' or 0, that neuron would stay turned off and would not activate any of the other connected neurons. However if the value was 0.51, than the neurons would be 'activated'.

Lets talk about how weights effect Hidden layers. Remember hidden layers, like input layers, are collections of neurons that are connected to the neurons in the next layer. For example if we had 2 hidden layers, the first with 1 neuron and the second with 5. The weights of the network would be the strength of each connection between the neuron in the first layer to all of the neurons in the second layer. The strength of each connection will effect the value of the neuron and how it reacts to the data it receives which in turn will effect the next neuron down the line.

Now I hope its clear how Neural networks can be complicated and simple simultaneously. As an example if we had a network with a input layer of 100 neurons, and 5 hidden layers of 50 neurons and an output layer of 10 neurons, we could calculate the total number of weights like this: (100 x 50 + 50) + (50 x 50 + 50) + (50 x 50 + 50) + (50 x 50 + 50) + (50 x 50 + 50) + (50 x 50 + 50) + (50 x 10 + 10) = 18310 different weights and that would be for an image with only 100 pixels!
