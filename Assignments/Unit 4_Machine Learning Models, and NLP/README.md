Neural Network Basics Summary
In this Unit we began to explore Neural networks and the types of problems they can be used to solve. Simple machine learning algorithms work best on problems of classification which makes them useful for identification of complex patterns such as voice recognition, image recognition, text prediction and many others. We started by exploring natural language processing so that we could further understand how these algorithms work on a basic level before exploring more complicated topics. 

The most recognizable Natural language processing is the autocorrect or text prediction that keeps us from making spelling errors while texting or writing. It is fairly simple to imagine how a model is trained to recognize spelling mistakes. If we think of a word as a pattern of letters, take for example the word 'fish', the word itself is made of 4 unique letters in a unique order. As humans we immediately know what this word says and means because we have associated the word with the sound each letter makes and then with the animal. This concept is essentially what we need to teach our model to do. To do this we first we have to teach the model what the correct spelling (or pattern) of every word so that it can recognize when it doesn't know a pattern. Once it sees that we have made a word it doesn't recognize, the next step is compare the unknown word to its dataset and find the closest matching word. Maybe you only missed 1 letter, 'Fush' is not an English word but we can see how it easily could be recognized as a misspelling of 'Fish'. 

In most applications machine learning is just pattern matching and classification. The 5 major components of a Neural network are Neurons, Weight and Bias parameters, an activation function, a loss function and input/output layers. To further our understanding we explored the MNIST hand written digit dataset. This dataset consists of many 28x28 pixel images of a hand written digits between 0 - 9. To begin to match the patterns in this dataset we need to understand how the first layer of a neural network functions. This layer is called the "input layer" and is a collection of "Neurons" which is a single number usually between -1 and 1. Each Neuron is responsible for reacting to a single piece of data. This means that for our 28x28 pixel images our first layer needs to have 784 neurons, one for each pixel or piece of data. Each following layer is called a Hidden Layer. The last layer in the network is called the Output layer and needs to have as many neurons as we have classification options for our images. In this case we have images with numbers ranging from 0-9 so we have 10 neurons in our output layer. 

How does the network know what each image represents? good question, in the beginning it doesn't. This is where we train the network to recognize patterns. In classification problems also called supervised learning, we have already classified some of the images in our dataset. When we feed the network an image it accepts the data into its input layer, then, surprisingly, creates a bunch of random noise. This random noise results in an incoherent output which is where weights and activation functions come in. To teach the network which number each image is supposed represent, the network takes each random output and adjusts the weights for each neuron in the hidden layers till the output matches our classified image. The network does this for every single image in the dataset and creates a extremely complex pattern that can recognize new images it has never seen before with a relatively decent accuracy.

Now lets talk about activation functions and how it effects the neurons in the network. An activation function is a function that is used to decide at what point each neuron "activates" and passes along data. For example if we had a neuron whose value was 0.49 and our activation function was set up so that any number below .50 was 'Off' or 0, that neuron would stay turned off and would not activate any of the other connected neurons. However if the value was 0.51, than the neurons would be 'activated'. 

Lets talk about how weights effect Hidden layers. Remember hidden layers, like input layers,  are collections of neurons that are connected to the neurons in the next layer. For example if we had 2 hidden layers, the first with 1 neuron and the second with 5. The weights of the network would be the strength of each connection between the neuron in the first layer to all of the neurons in the second layer. The strength of each connection will effect the value of the neuron and how it reacts to the data it receives which in turn will effect the next neuron down the line. 

Now I hope its clear how Neural networks can be complicated and simple simultaneously. As an example if we had a network with a input layer of 100 neurons, and 5 hidden layers of 50 neurons and an output layer of 10 neurons, we could calculate the total number of weights like this: (100 x 50 + 50) + (50 x 50 + 50) + (50 x 50 + 50) + (50 x 50 + 50) + (50 x 50 + 50) + (50 x 50 + 50) + (50 x 10 + 10) = 18310 different weights and that would be for an image with only 100 pixels!
